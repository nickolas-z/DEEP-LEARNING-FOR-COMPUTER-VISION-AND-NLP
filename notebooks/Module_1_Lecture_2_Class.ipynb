{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bea5e27",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Module 1. Structured data processing using neural networks\n",
    "</h1>\n",
    "<h2 style=\"text-align: center;\">Topic 2. Logistic regression. Artificial neuron</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc12165",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel '.venv (Python 3.12.3)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. WebSocket is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "# filter warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637cc41c",
   "metadata": {},
   "source": [
    "## Dataset owerview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca655494",
   "metadata": {},
   "source": [
    "[Spaceship Titanic dataset](https://www.kaggle.com/competitions/spaceship-titanic/overview)\n",
    "\n",
    "**Data Field Description:**\n",
    "* `PassengerId` - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n",
    "* `HomePlanet` - The planet the passenger departed from, typically their planet of permanent residence.\n",
    "* `CryoSleep` - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n",
    "* `Cabin` - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n",
    "* `Destination` - The planet the passenger will be debarking to.\n",
    "* `Age` - The age of the passenger.\n",
    "* `VIP` - Whether the passenger has paid for special VIP service during the voyage.\n",
    "* `RoomService`, `FoodCourt`, `ShoppingMall`, `Spa`, `VRDeck` - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n",
    "* `Name` - The first and last names of the passenger.\n",
    "* `Transported` - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b4ac9",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d4b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "\n",
    "df = pd.read_csv('../data/Module_1_Lecture_2_Class_Spaceship_Titanic.csv')\n",
    "df = df.set_index('PassengerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426ed594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset preview\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45858db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of records and columns\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32505fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset info \n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f32a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'Transported'\n",
    "FEATURES = [col for col in df.columns if col != TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48be4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = [\"Cabin\", \"Name\"]\n",
    "cat_features = [col for col in FEATURES if df[col].nunique() < 25 and col not in text_features ]\n",
    "cont_features = [col for col in FEATURES if df[col].nunique() >= 25 and col not in text_features ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac31fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of categorical features: {len(cat_features)}')\n",
    "print('Categorical features:', cat_features, '\\n')\n",
    "print(f'Number of continuos features: {len(cont_features)}')\n",
    "print('Continuos features:', cont_features, '\\n')\n",
    "print(f'Number of text features: {len(text_features)}')\n",
    "print('Text features:', text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3058ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[TARGET].value_counts().plot(kind='bar', figsize=(8, 5))\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i)\n",
    "    ax.set_xlabel(\"value\")\n",
    "    ax.set_ylabel(\"count\")\n",
    "            \n",
    "plt.suptitle(\"Target feature distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06c22d",
   "metadata": {},
   "source": [
    "We can see that the target feature is distributed equally. That means we can use accuracy as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655c0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous features\n",
    "\n",
    "ax = df.loc[:, cont_features].hist(figsize=(10, 12), grid=False, edgecolor='black', linewidth=.4)\n",
    "for row in ax:\n",
    "    for col in row:\n",
    "        for i in col.containers:\n",
    "            col.bar_label(i)\n",
    "            col.set_xlabel(\"value\")\n",
    "            col.set_ylabel(\"count\")\n",
    "        \n",
    "plt.suptitle(\"Continuous features distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6da7be8",
   "metadata": {},
   "source": [
    "It looks like most of the passengers didn't use luxury services - all luxury amenities plots are heavily left-skewed. Let's create binary features for this services: if passanger used the service or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb72938",
   "metadata": {},
   "outputs": [],
   "source": [
    "services_features = cont_features[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1829bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in services_features:\n",
    "    df[f'used_{feature}'] = df.loc[:, feature].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ca79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "\n",
    "df.loc[:, cont_features + ['CryoSleep', 'VIP', TARGET]].corr().style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca71107",
   "metadata": {},
   "source": [
    "We can see that input features are not correlated with oneself. So we won't drop any of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb7f04a",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f33bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing Missing Values \n",
    "\n",
    "imputer_cols = [\"Age\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\" ,\"RoomService\"]\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(df[imputer_cols])\n",
    "df[imputer_cols] = imputer.transform(df[imputer_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"HomePlanet\"].fillna('Gallifrey', inplace=True)\n",
    "df[\"Destination\"].fillna('Skaro', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1db6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CryoSleep_is_missing'] = df['CryoSleep'].isna().astype(int)\n",
    "df['VIP_is_missing'] = df['VIP'].isna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952401cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df['CryoSleep'].value_counts())\n",
    "display(df['VIP'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51bf74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CryoSleep\"].fillna(False, inplace=True)\n",
    "df[\"VIP\"].fillna(False, inplace=True)\n",
    "\n",
    "df[\"CryoSleep\"] = df[\"CryoSleep\"].astype(int)\n",
    "df[\"VIP\"] = df[\"VIP\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03969295",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(df.loc[:, ['HomePlanet', 'Destination']], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331ad42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0185f7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, dummies], axis=1)\n",
    "df.drop(columns=['HomePlanet', 'Destination'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ff9505",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[TARGET] = df[TARGET].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea8d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, dropping textual features\n",
    "\n",
    "df.drop([\"Name\" ,\"Cabin\"] , axis=1 ,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab8d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "\n",
    "X = df.drop(TARGET , axis =1 )\n",
    "y = df[TARGET]\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(X, y, random_state = 42, test_size =0.33, stratify=y)\n",
    "# use stratify=y to ensure that the distribution of the target variable will be preserved in train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ddeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For further computation purpuses, let's transpose input data\n",
    "\n",
    "x_train = X_train.T\n",
    "x_test = X_test.T\n",
    "y_train = np.expand_dims(y_train.T, 0)\n",
    "y_test = np.expand_dims(y_test.T, 0)\n",
    "\n",
    "print('X train size', x_train.shape)\n",
    "print('X test size', x_test.shape)\n",
    "print('y train size', y_train.shape)\n",
    "print('y test size', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b28ae",
   "metadata": {},
   "source": [
    "## Initializing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7073e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to initialize parameters\n",
    "\n",
    "def initialize_weights_and_bias(dimension):\n",
    "    # dimension - number of input features\n",
    "    w = np.full((dimension,1),0.01)\n",
    "    b = 0.0\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef945626",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    y_head = 1/(1+np.exp(-z))\n",
    "    return y_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca398db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(w,b,x_train,y_train):\n",
    "    z = np.dot(w.T,x_train) + b\n",
    "    y_head = sigmoid(z) # probabilistic 0-1\n",
    "    loss = -1*y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n",
    "    cost = (np.sum(loss))/x_train.shape[1]      # x_train.shape[1]  is for scaling\n",
    "    return cost "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3408a6d6",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654d4ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In backward propagation we will use y_head that found in forward progation\n",
    "# Therefore instead of writing backward propagation method, lets combine forward propagation and backward propagation\n",
    "\n",
    "def forward_backward_propagation(w,b,x_train,y_train, eps=1e-5):\n",
    "    \n",
    "    # forward propagation\n",
    "    \n",
    "    z = np.dot(w.T,x_train) + b\n",
    "    y_head = sigmoid(z)\n",
    "    loss = -1*y_train*np.log(y_head+eps)-(1-y_train)*np.log(1-y_head+eps)\n",
    "    cost = (np.sum(loss))/x_train.shape[1]      # x_train.shape[1]  is for scaling\n",
    "    \n",
    "    # backward propagation\n",
    "    \n",
    "    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n",
    "    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n",
    "    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n",
    "    \n",
    "    return cost,gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a63b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating(learning) parameters\n",
    "\n",
    "def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n",
    "    cost_list = []\n",
    "    index = []\n",
    "    \n",
    "    # updating(learning) parameters is number_of_iterarion times\n",
    "    \n",
    "    for i in range(number_of_iterarion):\n",
    "        \n",
    "        # make forward and backward propagation and find cost and gradients\n",
    "        \n",
    "        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n",
    "        cost_list.append(cost)\n",
    "        \n",
    "        # lets update\n",
    "        \n",
    "        w = w - learning_rate * gradients[\"derivative_weight\"]\n",
    "        b = b - learning_rate * gradients[\"derivative_bias\"]\n",
    "        index.append(i)\n",
    "        \n",
    "    # we update(learn) parameters weights and bias\n",
    "    \n",
    "    parameters = {\"weight\": w,\"bias\": b}\n",
    "    plt.plot(index,cost_list)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterarion\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters, gradients, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba0bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,b,x_test):\n",
    "    \n",
    "    # x_test is a input for forward propagation\n",
    "    \n",
    "    z = sigmoid(np.dot(w.T,x_test)+b)\n",
    "    Y_prediction = np.zeros((1,x_test.shape[1]))\n",
    "    \n",
    "    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n",
    "    # if z is smaller than 0.5, our prediction is sign zero (y_head=0)\n",
    "    \n",
    "    for i in range(z.shape[1]):\n",
    "        if z[0,i]<= 0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 1\n",
    "\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0abd40",
   "metadata": {},
   "source": [
    "Lets put them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e33695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n",
    "    \n",
    "    # initialize\n",
    "    \n",
    "    dimension =  x_train.shape[0]  # that is 4096\n",
    "    w,b = initialize_weights_and_bias(dimension)\n",
    "    \n",
    "    # do not change learning rate\n",
    "    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n",
    "    \n",
    "    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n",
    "    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.00001, num_iterations = 50) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
